---
layout: page
title: "话题聊配对系统方案设计"
categories:
- 系统 技术方案
tags:
- 系统设计
---



----

   去年年后接到一个独立系统，如下所示，需要根据某些话题为
两个不认识的人进行配对，配对上可以就该话题两个人交换意见聊天，
配对等待太久会结合用户属性出推荐话题。
大体样式如下图的两人牵线所示:
![话题](http://7xp7tl.com1.z0.glb.clouddn.com/subject.png)

![发起配对](http://7xp7tl.com1.z0.glb.clouddn.com/makepair.png)

![配对结束](http://7xp7tl.com1.z0.glb.clouddn.com/chat.png)


----


----


什么？你逗我？这太简单吧？这玩意也能叫一个系统？
我后台开个进程，每个话题开一个队列。看下队列有人就匹配上，没人就入队列
等着别人来跟他匹配。分分钟给你搞出来！
那么问题来了。

1.  用户在等待配对过程得通知后台吧，切出话题不想配对了也得通知后台把
用户从话题的等待配对队列里剔除。但是逻辑层是多台机子无状态的，如果退
出话题的请求先于心跳请求到达后台。那么用户还是会插入等待队列中被其他
用户匹配上。这个时序问题如何处理？
2. 匹配上了如何通知被匹配的一方?常规的通知是通过push下发，如果被配对的
对象刚好断网或者下线了呢？弱网络下如何保证基本体验？

3.  系统估计1亿用户百万在线。每个话题分十万人，后台架构如何设计才能
支撑可能存在的扎堆请求配对的问题？

4.  配对上的规则需要自由订制，例如男同女同话题只允许同性进入，暧昧话题
优先配对异性伙伴。星座话题优先配对相匹配星座伙伴，方案如何设计才能方便产品和运营需要？
。。。。。。

上面的问题考虑过没？

----

####下面先来聊下大概的一个方案设计问题。


----
前面陌生人群组的架构已经描述了整体的架构概况。而匹配服务
刚好是落在重型逻辑svr这一层。

最简单的方式是重型逻辑svr这一层做一个cache。然后每个话题建立一个队列。
每次配对请求到达的时候从cache中取出队列信息进行匹配。为了能在重启之后
快速恢复原来缓存中的信息，使用共享内存来管理cache。但是这种处理会带来
单点的问题。一旦匹配服务svr 挂机了就无法正常提供匹配服务。而我们做服务
的原则之一是保证成功率99.99% 以上。

因此需要在重型逻辑svr这一层加一个proxy。搞双机备份。但是关于cache的备份
又会带来数据同步的问题，是增量数据的时时同步，还是定时的同步。同步失败了
怎么处理？需要在proxy增加判定策略来判断服务是否可用。这套问题解决好了还有
新的问题。一旦匹配服务扛不住压力要扩容怎么办？很显然这套方案不利于平行扩展。
怎么降低这套东西的运维成本？这些是上面第一套方案需要面对的问题。



既然上面的方案对运维，扩容，以及版本升级的处理不是很友好，处理成本
相对较高。那我们来看看有什么优化的点。


之所以会出现上述问题，是因为逻辑层有状态，那将逻辑层处理成无状态，
状态保存在数据层。这样子可以直接使用基于内存的成熟数据库组件。但这种处理
方式会带来数据一致性问题。（分布式情况下访问相同的话题队列）。这种情况
需要怎么处理？遇到这种问题需要从业务数据模型出发。


按照以往产品得到的数据模型统计。从时间上分布同一个话题大概峰值是每秒1000
次配对操作。而db内网一次数据往返时延是10ms。运算下来出现数据冲突的概率在1%左右，
如果增加二次重试是0.01%（这里的估算是有问题的！）。因此把状态保存到db层这种
处理方式完全可以满足业务的需要。



----


####重新再回到刚才聊到的话题


----

1. 配对时序问题。进入配对之后，每隔5s钟会有一次配对心跳到达。来确保用户还是
在当前页面等待配对。如下所示：如果A发起配对请求进入等待队列了，在心跳发出来
不久用户选择退出该话题配对了。客户端是采用请求入单队列的形式。请求从用户到
接入机还是有序的，但是请求从轻逻辑层开始就无法保证有序了。因此在处理配对请
求的时候，退出话题配对的请求到达，剔除等待用户。心跳请求稍后到达，又将本已经
退出话题的用户压入等待队列中。
最直观的体验是。我等了一会，没有等到配对对象，结果退出话题，一会客户端又显示配对上了。
具体情况如下所示：

----
