---
layout: page
title: "陌生人群组项目架构浅述"
categories:
- 架构 整体方案 
tags:
- 架构
---



----

   去年还在做社交产品后台。过了不到半年原部门就被解散了，如今入职游戏已经半年有余了。
昨天跟一个技术总监闲聊，他问了我下过去都做过什么。摸摸脑袋竟然有好些细节都遗忘了。
趁着入职新项目组还有一周的交接时间，把过去的做过的服务挑出来。做下陈述总结。


----

----
   去年做过的服务是一个陌生人群聊的服务。主打兴趣社交，有群组聊天，有lbs 活动，
有话题速配聊天等服务。而我恰好都接触一些。 今天就先简单陈述下后台的整体架构和涉及到的一些细节.
服务器的设计和部署是冲着海量用户去的。在北京，上海，深圳三地都设置有接入点。处理后台集中部署在上海。
陈述前先细述下产品的策略架构。如下所示：
![群聊粗略架构](http://7xp7tl.com1.z0.glb.clouddn.com/微群组服务器大略架构.png)


----




###首次登陆处理流程是这样的：

----
1.  客户端会写死一个域名。登陆的时候会访问域名所在的Mapping Server 映射服务器。
映射服务器会根据用户传上来的ip，网络运营商信息来给用户推荐最优接入的服务器ip列表
（统一接入网关的服务器ip）。这些最优接入的ip是由玩家跑马分析出来的，这里不做展开。
2. 客户端在拿到最优接入ip之后，发起tcp连接操作。连接上统一网关之后。统一网关会将
用户请求转发给长连接服务器。  
3. 长连接服务器本地会做一些简单处理（黑名单用户过滤，鉴权）。鉴权通过之后会将请求转发给轻逻辑服务器。
轻逻辑服务这一层有多个服务器。转发给哪台服务器在长连接服务器这里会有一个决策逻辑，
当然这个决策逻辑最好是配置化的，具体算法由独立模块负责。（长连接服务器需要稳定少升级）
长连接服务器采用的libevent来管理网络连接。
4. 轻逻辑服务器主要是做数据装配。各种版本兼容的擦屁股活。如果接口逻辑不重，
那么接口的实现也会做在这一层。时间久了代码看起来像一坨屎。这一层蛮要求编码规范约束和框架约束的，
否则到后面只想重构根本不想在上面继续迭代。轻逻辑层是面向用户最近的业务层，
通信上采用protocol  buffer协议，会负责请求各个api然后将客户端要求的数据组装起来返还给用户。
5. 如果接口逻辑比较复杂，涉及到多模块的数据处理，这时候就会按照功能来划分模块。
每个模块负责实现独立特定的业务逻辑。这些业务逻辑属于重型svr这一层。为了避免不同业务之间相互干扰，
这一层不同功能模块的服务都是分开部署的。如果混合部署会出现各种相互干扰的问题。
（某些功能模块做了运营活动或是具有节日特征，会导致请求量激增。如果混合部署可能会导致其他模块拥有的计算资源被干扰，
一个稳健系统的一个重要条件就是柔性可用，某个模块有问题不会波及到其他模块）
6. 数据层采用的是高速key/value 存储。如果部署在同一个园区内。从逻辑层到数据层的访问时延 一次往返在10ms内，基本可以满足业务的需要。
----


###以上架构中有几个细节不得不提:
----
1. 轻逻辑层和重逻辑层不缓存任何用户数据，每个请求到来之后都是立马从数据库拉取最新数据进行处理。状态只保存在数据层。这种处理方式的好处是可以无障碍平行扩容。全程不需要开发介入，运维改改配置就能迅速增大系统的处理能力
2. 第一点提到的逻辑处理层无状态会带来数据一致性的问题。对于一些共享数据访问可能会出现冲突的问题。但是社交应用不像银行系统需要高度的一致性。因此综合可维护性，性能采用了最终一致性的数据处理方式。处理数据的时候采用cas版本号校验。一旦出现冲突，让请求失败即可。这种失败的概率很小，对玩家体验造成的损失几乎可以忽略
3. 为了提高系统的鲁棒性。层与层之间进行请求转发的时候必须摆脱对于物理地址的依赖。举个例子，轻逻辑层有多台服务器，重逻辑层也有多台服务器。但是轻逻辑层的服务器对后端的重逻辑层服务器发起一个网络请求的时候，只会选择一台。最简单的处理方式是维护一堆物理ip，每次负载均衡出一个ip。然后连接。这种方式最大的弊端就是后端服务器挂了会影响线上的请求。等到告警短信来了再手动刷配置下掉服务效率慢不说，也影响大家的日常作息。
 

----

###业界的经典处理方法如下:


----
    在主调和被调之间加一层proxy代理层。主调方只要无脑转发给proxy层就行。
Proxy这里会进行负载均衡 并会检测每个请求的错误情况。如果是非逻辑错误超过
一定阀值，或者定期发送给被调方的心跳没有及时 收到回包，就会告警并且自动将
被调方的故障服务器自动屏蔽一段时间，过段时间再重试下决策是否重新加入服务列表里。
![代理转发](http://7xp7tl.com1.z0.glb.clouddn.com/proxy.PNG)

    不过项目采用的是我见过的最优雅的处理方式。也就是上图架构中 的虚拟地址到物理地址的映射服务。
![虚拟地址到物理地址转换服务](http://7xp7tl.com1.z0.glb.clouddn.com/L.PNG)

----



###工作机理是：

----
    在每个服务器上维护一个agent。这个agengt负责接受映射中心推送过来的映射结果。
映射结果 是key=》ip，port。 主调方只要采用调用映射服务的api，传入key，
可以自动获取到ip，port。这个ip和port 是agent 计算出来的结果， 兼顾了负载均衡的逻辑。
每个请求调用结束之后都会上报业务错误码给agent，由agent 进行决策看是否是后端某个服务出故障了。
当然agent也会定期发送心跳给后端服务器。Agent得到的服务器检测结果会上报给映射中心。
映射中心再统一决策修改映射规则并下发给各个agent。以上处理方式把传统的proxy从通信链路里剥离出来。

----



----
    主调方采用滞后反馈的结果才连接后端，这种处理方式将负载均衡的功能处理成旁路逻辑。
提升了性能，而且后端的运维升级完全对前端来说事透明的。 当然有人会说以上的架构不就是接入层，
逻辑层，数据层这套东西吗。凭什么你这套架构就能够处理海量服务。但我想纠正一点
海量的服务体现在系统的各个细节处理上。而不是这些大概的层级划分。

----


----
    从系统处理上，每一层都有精细化的处理。举几个简单的例子：
1. 在接入层这里会做最大连接数限制.
2. 轻逻辑层这里的网络连接采用的是proxy，worker的处理模型。请求会以队列的形式进行排队处理。进队列的时候都会打上一个时间戳。如果超过队列缓存的最大个数，新来的请求会直接丢弃。如果从队列中获取到的请求等待时间过长也直接丢弃。
3. 客户端这里也会谨慎重试。如果发现后端返回的错误码显示过载，会采用N步回退的重试策略，或者是出动画或者提示降低用户的焦躁感避免疯狂重试。
4. 对于每个请求的成功，失败，返回码。时延，错误码分布，时间，qua这些维度的信息，都会在框架api里进行统一的封装上报。可以在监控平台里快速查询到接口在某个具体时间段的处理情况。如果超过一定阀值会进行短信，微信轰炸，第一时间通知到责任人
5. 对于每台服务器都有流量，CPU，磁盘IO监控。对于每个涉及到用户关键数据的修改请求都有流水记录。方便出现问题的时候快速获取上下文处理。

----

----
    以上处理方式适用于社交相关的应用。总体思想是先扛住再优化，架构上支持快速扩容，
柔性可用。在有限的资源支撑下快速应对随时会爆发的互联网海量请求服务。
    写得太书面了，最近文笔退却好多争取以后写得幽默风趣点啊！
----

