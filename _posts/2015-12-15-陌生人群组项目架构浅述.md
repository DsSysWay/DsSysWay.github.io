---
layout: page
title: "陌生人群组项目架构浅述"
categories:
- 架构 整体方案 
tags:
- 架构
---



----

   去年还在做社交产品后台。过了不到半年原部门就被解散了，如今入职游戏已经半年有余了。
昨天跟一个技术总监闲聊，他问了我下过去都做过什么。摸摸脑袋竟然有好些细节都遗忘了。
趁着入职新项目组还有一周的交接时间，把过去的做过的服务挑出来。做下陈述总结。


----

----
   去年做过的服务是一个陌生人群聊的服务。主打兴趣社交，有群组聊天，有lbs 活动，
有话题速配聊天等服务。而我恰好都接触一些。 今天就先简单陈述下后台的整体架构和涉及到的一些细节.
服务器的设计和部署是冲着海量用户去的。在北京，上海，深圳三地都设置有接入点。处理后台集中部署在上海。
陈述前先细述下产品的策略架构。如下所示：
![群聊粗略架构](http://7xp7tl.com1.z0.glb.clouddn.com/微群组服务器大略架构.png)


----




###首次登陆处理流程是这样的：

----
1.  客户端会写死一个域名。登陆的时候会访问域名所在的Mapping Server 映射服务器。
映射服务器会根据用户传上来的ip，网络运营商信息来给用户推荐最优接入的服务器ip列表
（统一接入网关的服务器ip）。这些最优接入的ip是由玩家跑马分析出来的，这里不做展开。
2. 客户端在拿到最优接入ip之后，发起tcp连接操作。连接上统一网关之后。统一网关会将
用户请求转发给长连接服务器。  
3. 长连接服务器本地会做一些简单处理（黑名单用户过滤，鉴权）。鉴权通过之后会将请求转发给轻逻辑服务器。
轻逻辑服务这一层有多个服务器。转发给哪台服务器在长连接服务器这里会有一个决策逻辑，
当然这个决策逻辑最好是配置化的，具体算法由独立模块负责。（长连接服务器需要稳定少升级）
长连接服务器采用的libevent来管理网络连接。
4. 轻逻辑服务器主要是做数据装配。各种版本兼容的擦屁股活。如果接口逻辑不重，
那么接口的实现也会做在这一层。时间久了代码看起来像一坨屎。这一层蛮要求编码规范约束和框架约束的，
否则到后面只想重构根本不想在上面继续迭代。轻逻辑层是面向用户最近的业务层，
通信上采用protocol  buffer协议，会负责请求各个api然后将客户端要求的数据组装起来返还给用户。
5. 如果接口逻辑比较复杂，涉及到多模块的数据处理，这时候就会按照功能来划分模块。
每个模块负责实现独立特定的业务逻辑。这些业务逻辑属于重型svr这一层。为了避免不同业务之间相互干扰，
这一层不同功能模块的服务都是分开部署的。如果混合部署会出现各种相互干扰的问题。
（某些功能模块做了运营活动或是具有节日特征，会导致请求量激增。如果混合部署可能会导致其他模块拥有的计算资源被干扰，
一个稳健系统的一个重要条件就是柔性可用，某个模块有问题不会波及到其他模块）
6. 数据层采用的是高速key/value 存储。如果部署在同一个园区内。从逻辑层到数据层的访问时延 一次往返在10ms内，基本可以满足业务的需要。

----


----
    请求流向有两个：
1. 从长连接服务器->轻逻辑层服务->重逻辑服务<->db.常规请求类似于拉取个人资料，群资料会按照这个路径来走并原路返回
2. 聊天消息会触发一个在线的推送处理。假设A用户在群里说了句话，由此引发的请求流向路径如下:
   A用户接入机->轻逻辑层服务－>重逻辑服务层（聊天svr，做群聊计数修改，权限校验，敏感词检测，数据落地） 
   ->push  svr(查找在线列表，获取用户接入的长连接服务器地址，组装推送包)->各个用户的接入机->到达终端

----


###以上架构中有几个细节不得不提:
----
1. 轻逻辑层和重逻辑层不缓存任何用户数据，每个请求到来之后都是立马从数据库拉取最新数据进行处理。状态只保存在数据层。这种处理方式的好处是可以无障碍平行扩容。全程不需要开发介入，运维改改配置就能迅速增大系统的处理能力
2. 第一点提到的逻辑处理层无状态会带来数据一致性的问题。对于一些共享数据访问可能会出现冲突的问题。但是社交应用不像银行系统需要高度的一致性。因此综合可维护性，性能采用了最终一致性的数据处理方式。处理数据的时候采用cas版本号校验。一旦出现冲突，让请求失败即可。这种失败的概率很小，对玩家体验造成的损失几乎可以忽略
3. 为了提高系统的鲁棒性。层与层之间进行请求转发的时候必须摆脱对于物理地址的依赖。举个例子，轻逻辑层有多台服务器，重逻辑层也有多台服务器。但是轻逻辑层的服务器对后端的重逻辑层服务器发起一个网络请求的时候，只会选择一台。最简单的处理方式是维护一堆物理ip，每次负载均衡出一个ip。然后连接。这种方式最大的弊端就是后端服务器挂了会影响线上的请求。等到告警短信来了再手动刷配置下掉服务效率慢不说，也影响大家的日常作息。
 

----

###业界的经典处理方法如下:


----
    在主调和被调之间加一层proxy代理层。主调方只要无脑转发给proxy层就行。
Proxy这里会进行负载均衡 并会检测每个请求的错误情况。如果是非逻辑错误超过
一定阀值，或者定期发送给被调方的心跳没有及时 收到回包，就会告警并且自动将
被调方的故障服务器自动屏蔽一段时间，过段时间再重试下决策是否重新加入服务列表里。
![代理转发](http://7xp7tl.com1.z0.glb.clouddn.com/proxy.PNG)

    不过项目采用的是我见过的最优雅的处理方式。也就是上图架构中 的虚拟地址到物理地址的映射服务。
![虚拟地址到物理地址转换服务](http://7xp7tl.com1.z0.glb.clouddn.com/L.PNG)

----



###工作机理是：

----
    在每个服务器上维护一个agent。这个agengt负责接受映射中心推送过来的映射结果。
映射结果 是key=》ip，port。 主调方只要采用调用映射服务的api，传入key，
可以自动获取到ip，port。这个ip和port 是agent 计算出来的结果， 兼顾了负载均衡的逻辑。
每个请求调用结束之后都会上报业务错误码给agent，由agent 进行决策看是否是后端某个服务出故障了。
当然agent也会定期发送心跳给后端服务器。Agent得到的服务器检测结果会上报给映射中心。
映射中心再统一决策修改映射规则并下发给各个agent。以上处理方式把传统的proxy从通信链路里剥离出来。

----



----
    主调方采用滞后反馈的结果才连接后端，这种处理方式将负载均衡的功能处理成旁路逻辑。
提升了性能，而且后端的运维升级完全对前端来说事透明的。 当然有人会说以上的架构不就是接入层，
逻辑层，数据层这套东西吗。凭什么你这套架构就能够处理海量服务。但我想纠正一点
海量的服务体现在系统的各个细节处理上。而不是这些大概的层级划分。

----


----
    从系统处理上，每一层都有精细化的处理。举几个简单的例子：
1. 在接入层这里会做最大连接数限制.
2. 轻逻辑层这里的网络连接采用的是proxy，worker的处理模型。请求会以队列的形式进行排队处理。进队列的时候都会打上一个时间戳。如果超过队列缓存的最大个数，新来的请求会直接丢弃。如果从队列中获取到的请求等待时间过长也直接丢弃。
3. 客户端这里也会谨慎重试。如果发现后端返回的错误码显示过载，会采用N步回退的重试策略，或者是出动画或者提示降低用户的焦躁感避免疯狂重试。
4. 对于每个请求的成功，失败，返回码。时延，错误码分布，时间，qua这些维度的信息，都会在框架api里进行统一的封装上报。可以在监控平台里快速查询到接口在某个具体时间段的处理情况。如果超过一定阀值会进行短信，微信轰炸，第一时间通知到责任人
5. 对于每台服务器都有流量，CPU，磁盘IO监控。对于每个涉及到用户关键数据的修改请求都有流水记录。方便出现问题的时候快速获取上下文处理。
6. 对于功能相近的请求做抽象，进行并包处理。减少网络请求的耗时（国内跨地区的时延往返在100ms左右，不算上网络延迟堵塞这种情况）
----


----
###系统设计关键点：

####1.用户在线列表是如何维护的
上海深圳北京三地用户接入，都会设置用户的在线状态值。这个标记值存储在上海的db中，
同时存储接入机的ip和端口地址。此后，客户端会定期的发送心跳给接入机，接入机透传
给推送svr。这个心跳的间隔大概在5分钟到半小时不等。一旦推送svr在超过两个
心跳以上的时间间隔里没有收到客户端的心跳数据，就会认为是客户端掉线了。
这种情况下推送svr会设置db中的用户在线状态为 下线，同时清理纪录的接入机的
ip端口数据。用户群聊的时候会批量拉取群内用户的在线状态, 同时获取用户的接入机ip和端口，
除此之外还会读取最后一次刷新的心跳时间。 只有在线状态被标记
并且心跳时间的才会执行推送。以上对于用户的在线状态数据是由接入机进行通知维护的，一旦接入机挂机。就只能等
用户重连到新接入机再进行更新。所以，在线状态数据中还有一个辅助手段是用户的心跳时间，推送的时候一旦
判断到用户的最后一次心跳时间超时，也会设置用户下线并且不执行推送
设置这么长的一个心跳间隔也是有原因的。移动网络下用户可能会经历频繁的网络切换，
信号丢失等情况。预留足够的经验值时间来等待用户重连。目前这种机制推送的成功率在95%左右。
####2.用户在线列表是如何存储的
用户在线列表存储在一个内存级的key，value 数据库中。用户可以选择用memcache，也可以用redis。做好备份，避免单点故障。

####3.前端接入机出故障会对用户有什么影响，如何处理可能出现的风险
前端接入机是有状态的，一旦挂机，所有通过此接入机的用户都会掉线。这种情况下统一接入网关是有探测的。
会第一时间把故障机子下掉。用户重新登录接入该网关的时候，会自动将请求发送给其他接入机。从用户体验
上来说这种影响十分有限. 没有执行重新登录的用户，此时实质是在线不可达的。如果刚好有推送请求恰好读取了
用户过期的在线状态，并且还处于心跳时间间隔内。这种情况推送是会失败的。用户侧最直观的感受就是没有收到
推送的消息。作为辅助手段，在后台会存储用户的未读消息计数。等到下次玩家登录重连的时候会上传本地的已读消息
计数。通过比对之后再把没有拉取到的数据用主动拉取的形式进行同步

####4. 轻逻辑层，重逻辑层服务器挂机
在这一层有虚拟地址映射的监测服务，如果请求失败超过一定阀值，会自动将故障服务器从可服务列表里除名。
会影响一些用户的部分请求。由于这些层的服务器无状态，下次请求的时候会使用正常的服务器来处理.对用户来
说几乎是无感知的。

####5. 数据层挂机

数据层采用的是分布式的架构。实际服务的机器不止一台。分布式本身设计的核心理念就是容错处理 



----
----

    以上处理方式适用于社交相关的应用。总体思想是先扛住再优化,架构上支持快速扩容,
柔性可用。在有限的资源支撑下快速应对随时会爆发的互联网海量请求服务。
    写得太书面了，最近文笔退却好多争取以后写得幽默风趣点啊！
----

