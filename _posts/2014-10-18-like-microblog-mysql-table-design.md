---
layout: post
title: "类微博类产品mysql存储方案设计."
categories:
- db scheme 
tags:
- db



---



----

下午需求做完了，刚好其他同事在搞方案评审。做的是一个新的sns产品。所以去旁听了会。sns中涉及到好友动态查看，以及互粉关系链存储，评论这些功能。由于是节目订阅类， 产品的功能点有很多跟微博类似。之前使用公司的架构同样可以扛住海量用户的请求冲击。但是leader的想法是追求创业团队的小而美，快速迭代上线。用公司的框架太重了， 调用链路太长，开发和维护的成本都比较高。所以架构上采用了通用的lamp组件进行开发。问题来了，挖掘机哪家强.......哦，说错了，是mysql中的数据结构如何组织才能够支持 快速的查询和数据层的无障碍扩容。 

----
关系链中因为有关注，有粉丝，最简单粗暴的方式是把这些关系都存成一张表。表属性设置为fromid，toid，timestamp。fromid代表发起关注者的id，toid表示被关注对象的id。 我们暂时称这张表为A吧。如果要查一个人关注了谁，<pre><code>select toid from tablename where fromid=XXXX</code></pre>就行。如果要查一个人的粉丝是谁，


----
这么搞真的行么？用户获取好友动态的时候，不管是采用推送模式还是拉取模式，都需要去查一把关系链。推送模式得查粉丝关系链，拉取模式得查关注关系链。 进一个人的主页查看对方关注列表或者粉丝列表需要知道这些列表中的账号与自己的互粉关系同样需要查询这张表。可以说关系链是访问最频繁的数据之一。 对于百万级注册用户，假设平均每个人关注了100个人，那这里的关系记录有100个百万也就是1亿条记录。对于检索频繁而且数据量这样大的数据，存到一个表里对于mysql的性能 是个问题。业界常用的做法是分库分表。分表也有垂直分表和水平分表这些方式。对于关系链存储，最适合的是水平分表。将数据打散到多个表中来减低访问的压力。 最简单的分表方式可以按照fromid来hash。业务侧或者中间层维护好映射关系。


###这里又带来了另一个问题：
----
数据分表之后。如果要查找某个人的粉丝列表<pre><code>select  fromid  from tablename  where toid=XXX（XXX是用户id）</code></pre>，需要对所有表都发起sql查询。假设分了N张表。 有M个查询粉丝列表的请求那这样产生的请求量就有M*N个sql请求。拿这样的系统出去都没法见人啊。说白了就是查反向关系链成本比较高。解决方法是再开一个冗余的关系数据表。 表结构跟A表一样，我们暂时称作B吧。但是分表的时候是根据toid来进行水平分区的。查反向关系链的时候可以根据toid映射到对应的表里快速查找到用户的粉丝列表。


###采用这种方式又带来新的问题：
----
用户的一次关注操作需要在表A和表B中塞一条记录。这里本身是一个事务操作。当然我们可以以A表的数据为主。B表数据只要保证最终一致，定时同步就行。


###这样的关系链存储是否就是最优的？
----
想得美啊骚年。遥想当年使用微博，知乎的时候，刚注册就给你推荐一坨人，运营和产品要搞个一键关注，一关注就是几十上百个人啊。 采用这种存储方式带来的问题就是关注多少人就产生多少sql语句啊。而且因为搞了A，B两张表，就是2倍的sql语句啊。虽然关注的写量不比发SNS里发动态的写量 ，但是这里的数据表膨胀速度还是蛮快的。尤其是有高质量的用户活跃的时候，关注写操作比较多，半夜都要起来分表啊，做数据迁移。写操作的成本和维护的成本都比较高。 是否有一种解决方案可以减少sql的执行次数，降低数据表膨胀的速度？毛主席教导我们，把别人的经验变成自己的，他的本事就大了。 qq空间老版本用的也是mysql。关系链存储存成两张表，一张是follow关注表，一张是粉丝表（这里简称为方案二）。表结构属性如下：uid，uidlist。 uid是用户的id，uidlist是关注对象或者粉丝列表，打包成二进制数据存成BLOB。


###采用这种方式，好处有以下几点：
1. 原来一键关注多人的写操作可以整成一条语句，只要修改uidlist就行了。另外数据会更加紧凑。假设mysql存储采用B+树InnoDB 存储引擎，平均每人的关注和粉丝列表都是100人，采用Blob的方式B+树的宽度要比原来存到A，B表（这里简称为方案一）的宽度理论上小100倍。B+树的结点分裂也比方案一频率要小。在uidlist比较短的时候写入效率高。 
2. 检索效率高。只要检索到用户的id就可以把所有关注或者粉丝列表拉出来，而方案一由于是一条条记录，即使在fromid或者toid上建立索引。查找的时候需要拉出
一个或者多个区块的数据出来。


----
缺点：采用这种方式，当用户关注对象或者粉丝量大的时候，每次都要操作一坨数据。如果某个用户出现粉丝集聚的情况，每次更新该用户的数据的时候都要把整个粉丝list都拉出来， 再塞回去。如果uidlist里需要塞每个用户的操作时间戳，uid，就有8个字节。出现百万乃至千万个uid的时候每次拉取的数据会达到8M-80M。在删除的时候还要遍历这些uidlist再行删除。 判断用户是否在uidlist中也要做次百万千万级别的查找。为何空间适用的关系链存储就没问题。因为空间是熟人社交，没有粉丝名人的集聚效应。当然你会说不对啊， 小米的认证空间不就有上千万粉丝么。这里是因为认证空间采用的是B+树单纯存储这些关系链的，相当于又采用方案一的表结构来存储。没有哪种数据组织形式可以适用于一切情况。 业务侧个人的关注量很有限，但是粉丝数却有可能突破上百万上千万。我们可以采用混合方案的方式。对于粉丝数目超过百万的大户，我们可以采用方案一的结构来单独存储这些用户的粉丝关系链。
粉丝数目小于百万的，可以采用方案二来存储。方便数据的快速写入和读取。这种处理方式可以应对海量的关系链数据处理，但是无疑也相应的增加了业务逻辑的复杂性。但是相对于采用单纯的方案要好


----
SNS中涉及到另外一个问题，好友动态如何快速的拉取。这里有pull和push两种方式。pull主要是采用拉取的方式。用户每发表一个好友动态，都会插入到一个动态表中，我们叫做feeds吧。feeds的表结构设置为 <pre><code>author，content，timestamp,extra msg</code></pre>每次拉取的时候需要 
<pre><code>select * from feeds where feeds.author in(关注列表)</code></pre>
pull方案既要查询关系链，又要根据关系链去动态列表中过滤出关注对象的动态。做排序。检索的时候可以根据在增量feeds表中做数据检索。 这种方式实现简单，可以实现业务需求，但是操作时间复杂度达到O(M)*O(N),M是动态表中的记录长度，N是关注列表的长度。


----
push方式是再设置一张表，我们叫做friendfeeds吧，表结构设置为uid，feedslist[{feedsid1 : timestamp1},{feedsid2:timestamp2}] ，uid是用户id，feedslist是关注对象发的feedslist列表。 列表按照发布时间进行排序。feedslist可以只存好友动态的id。如果用户取消了对关注对象的关注，顺带删除feedslist中关注取消对象的动态信息就行了。用户刷新动态的时候只要查friendfeeds， 拉出feedslist。如果feedslist中存储的是动态的全量数据，那么拉取好友动态的时间复杂度是O(1).但是如果出现粉丝集聚，热点账号发布一条动态会带来恐怖的写扩散。上千万用户会带来上千万的写操作。 而且带来大量的数据冗余。用户修改动态数据或者app需要修改展现格式的时候修改成本也很大。退一步feedslist只存feeds的索引，拉取到feedslist之后再根据索引去拉动态数据。所有动态数据只存一份。 这种方式带来的拉取时间复杂度是O（M）（M是feedslist的长度）要修改动态的展现形式或者用户需要修改自己发布的动态的时候都只要动一份数据就行了。 只是只存索引的方式同样还是没法应对粉丝集聚带来的恐怖写扩散问题，之前处理过的相关的业务，单单针对热点账号存的写索引数据成本很惊人。


----
这里进一步做混合方案。push只针对热点用户，用户每次登陆的时候，都会去更新热点状态位，对于长时间没有登录的用户，热点状态位被重置。用户很长时间登录之后如果热点状态位没有被激活， 就用拉的方式去获取动态数据。另外如果查询到的热点状态位处于有效状态，则去friendfeeds表中拉取自己的好友动态。对于热点账号， 为了防止恐怖的写扩散,我们可以在friendfeeds数据表中加入新的属性如下：<pre><code>uid，content，feedslist[feedsid1,feedsid2....]，hotuidlist[uid1,uid2....]，timestamp</code></pre> feedslist存储的是关注人最近发布的动态id，按照发布时间排序。hotuidlist是自己关注的热门账户uid 列表。timestamp是用户上次拉取动作的时间戳。 另外有个hash表（简称动态时间hash）存储热点用户的最新的动态发表时间戳，最近一周内发布的动态的id和对应的时间戳，hash表结构表示为：
<pre><code>key：uid
value：最新动态时间戳，[{feedsid1:timestamp1},{feedsid2:timestamp2}.........]</code></pre>
每次热点用户发布新动态的时候都会去更新这张hash表的发表时间戳和id。用户在拉完friendfeeds   里普通关注人的feedslist之后，也会拉到关注的热点账号的uidlist和上次拉取的时间戳。 拿这个时间戳去动态时间hash表中比对下时间戳，如果不一致就根据时间戳过滤出hash表中的这些热门账号的feedsid，再把拿到的这些feedslist索引去feeds表中拉取真正的动态数据。 这样拉取动态的时间复杂度是O（M+N*T/2）M是feedslist长度，N是用户关注的热点账号，T是每个热点账号还未被用户拉取的动态数量。采用这种方案避免了push的写扩散， 相对pull先去查关系链再去遍历feeds表要快些，少了查关系链的sql操作。用户长时间没有登录，后台的定时任务可以重置热点状态，同时删除friendfeeds中关于用户的feedslist数据。 friendfeeds  和hash  这两个结构建议放在cache里，承载的组件可以用redis。


----
当然，对于一个新业务，快速上线才是王道，怎么简单怎么做。当请求量大之后架构上可以支持快速加机器扛住请求。做复合优化方案不需要对存储做大的变动。 个人觉得就是好的方案。第一次发文，有错误的地方欢迎拍砖~






