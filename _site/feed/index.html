<?xml version="1.0" encoding="utf-8"?>
  <rss version="2.0"
        xmlns:content="http://purl.org/rss/1.0/modules/content/"
        xmlns:atom="http://www.w3.org/2005/Atom"
  >
  <channel>
    <title>chriswei</title>
    <link href="http://192.168.73.128:4000/feed/" rel="self" />
    <link href="http://dssysway.github.io" />
    <lastBuildDate>2017-01-25T23:18:47-06:00</lastBuildDate>
    <webMaster>linuxserverlover@gmail.com</webMaster>
    
    <item>
      <title>话题聊配对系统方案设计</title>
      <link href="http://192.168.73.128:4000/2015/12/22/%E8%AF%9D%E9%A2%98%E8%81%8A%E9%85%8D%E5%AF%B9%E7%B3%BB%E7%BB%9F%E6%B5%85%E8%BF%B0/"/>
      <pubDate>2015-12-22T00:00:00-06:00</pubDate>
      <author>chriswei</author>
      <guid>http://192.168.73.128:4000/2015/12/22/话题聊配对系统浅述</guid>
      <content:encoded><![CDATA[<hr />

<p>   去年年后接到一个独立系统，如下所示，需要根据某些话题为
两个不认识的人进行配对，配对上可以就该话题两个人交换意见聊天，
配对等待太久会结合用户属性出推荐话题。
大体样式如下图的两人牵线所示:
<img src="http://7xp7tl.com1.z0.glb.clouddn.com/subject.png" alt="话题" /></p>

<p><img src="http://7xp7tl.com1.z0.glb.clouddn.com/makepair.png" alt="发起配对" /></p>

<p><img src="http://7xp7tl.com1.z0.glb.clouddn.com/chat.png" alt="配对结束" /></p>

<hr />

<hr />

<p>什么？你逗我？这太简单吧？这玩意也能叫一个系统？
我后台开个进程，每个话题开一个队列。看下队列有人就匹配上，没人就入队列
等着别人来跟他匹配。分分钟给你搞出来！
那么问题来了。</p>

<ol>
<li>用户在等待配对过程得通知后台吧，切出话题不想配对了也得通知后台把
用户从话题的等待配对队列里剔除。但是逻辑层是多台机子无状态的，如果退
出话题的请求先于心跳请求到达后台。那么用户还是会插入等待队列中被其他
用户匹配上。这个时序问题如何处理？</li>
<li><p>匹配上了如何通知被匹配的一方?常规的通知是通过push下发，如果被配对的
对象刚好断网或者下线了呢？弱网络下如何保证基本体验？</p></li>
<li><p>系统估计1亿用户百万在线。每个话题分十万人，后台架构如何设计才能
支撑可能存在的扎堆请求配对的问题？</p></li>
<li><p>配对上的规则需要自由订制，例如男同女同话题只允许同性进入，暧昧话题
优先配对异性伙伴。星座话题优先配对相匹配星座伙伴，方案如何设计才能方便产品和运营需要？
。。。。。。</p></li>
</ol>


<p>上面的问题考虑过没？</p>

<hr />

<h4>下面先来聊下大概的一个方案设计问题。</h4>

<hr />

<p>前面陌生人群组的架构已经描述了整体的架构概况。而匹配服务
刚好是落在重型逻辑svr这一层。</p>

<p>最简单的方式是重型逻辑svr这一层做一个cache。然后每个话题建立一个队列。
每次配对请求到达的时候从cache中取出队列信息进行匹配。为了能在重启之后
快速恢复原来缓存中的信息，使用共享内存来管理cache。但是这种处理会带来
单点的问题。一旦匹配服务svr 挂机了就无法正常提供匹配服务。而我们做服务
的原则之一是保证成功率99.99% 以上。</p>

<p>因此需要在重型逻辑svr这一层加一个proxy。搞双机备份。但是关于cache的备份
又会带来数据同步的问题，是增量数据的时时同步，还是定时的同步。同步失败了
怎么处理？需要在proxy增加判定策略来判断服务是否可用。这套问题解决好了还有
新的问题。一旦匹配服务扛不住压力要扩容怎么办？很显然这套方案不利于平行扩展。
怎么降低这套东西的运维成本？这些是上面第一套方案需要面对的问题。</p>

<p>既然上面的方案对运维，扩容，以及版本升级的处理不是很友好，处理成本
相对较高。那我们来看看有什么优化的点。</p>

<p>之所以会出现上述问题，是因为逻辑层有状态，那将逻辑层处理成无状态，
状态保存在数据层。这样子可以直接使用基于内存的成熟数据库组件。但这种处理
方式会带来数据一致性问题。（分布式情况下访问相同的话题队列）。这种情况
需要怎么处理？遇到这种问题需要从业务数据模型出发。</p>

<p>按照以往产品得到的数据模型统计。从时间上分布同一个话题大概峰值是每秒1000
次配对操作。而db内网一次数据往返时延是10ms。运算下来出现数据冲突的概率在1%左右，
如果增加二次重试是0.01%（这里的估算是有问题的！）。因此把状态保存到db层这种
处理方式完全可以满足业务的需要。</p>

<hr />

<h4>重新再回到刚才聊到的话题</h4>

<hr />

<ol>
<li>配对时序问题。进入配对之后，每隔5s钟会有一次配对心跳到达。来确保用户还是
在当前页面等待配对。如下所示：如果A发起配对请求进入等待队列了，在心跳发出来
不久用户选择退出该话题配对了。客户端是采用请求入单队列的形式。请求从用户到
接入机还是有序的，但是请求从轻逻辑层开始就无法保证有序了。因此在处理配对请
求的时候，退出话题配对的请求到达，剔除等待用户。心跳请求稍后到达，又将本已经
退出话题的用户压入等待队列中。
最直观的体验是。我等了一会，没有等到配对对象，结果退出话题，一会客户端又显示配对上了。
具体情况如下所示：</li>
</ol>


<p>针对这种情况可以采用以下方案来处理。
配对请求本身包含以下过程：
发起请求配对，定时发起心跳表示还在等待，退出话题配对。</p>

<p>针对以上过程可以生成一个标记clientkey。这个clientkey可以使用用户名+时间戳+随机数生成。
在上述这组过程的请求都打上同一个clientkey。一旦退出话题配对，则在后台标记该clientkey
下的存储标志位，表示用户已经退出该话题了。如果接收到相同clientKey的心跳，就直接丢弃。
下次用户重新进入该话题，使用新生成的clientkey即可</p>

<p>具体数据结构表示如下：
处理方式：</p>

<p>后台维持一个key为clientkey，value为一个标志位的存储。客户端携带一个clientkey。
首次心跳到达后台会存储clientkey的值为0标志请求已经进入队列。取消操作会存储
clientkey的值为1标志取消操作。
（1）用户每次心跳上来都会查看clientkey存储的值是否为1，为1则直接丢弃心跳请求。
（说明用户已经取消这次配对了）为0则更新心跳时间。
clientkey会依照用户的操作场景累积起来，采用数据库自动淘汰方式淘汰即可。可以设置为1D
上述过程就变为如下图所示效果</p>

<ol>
<li>匹配上了如何通知被匹配的一方？常规的通知是通过push下发，如果被配对的对象刚好断网
或者下线了呢？弱网络下如何保证基本体验？
 目前Android在线push成功率大概在95%左右，在客户端收到客户端的回应包之后才会
认定为是收到push了。但是移动网络下面条件复杂，用户可能会出入电梯，可能在高铁上，
可能在干扰环境或者是基站覆盖信号较弱的地区，随时可能掉线。单纯靠push无法解决通知
到对方的问题。另外两人配对的时候，成功了，已有的解决方案是同时给双方发push，
会存在时序问题。一方A收到push，另一方B没有收到，那么另外一方会持续发心跳，
此时另外一方不在等待队列了。如果不将B重入队列，就会存在B一直等在配对过程页
的问题，除非push到达才能解决。如果将B重入队列，就会存在B同另外的人匹配上的情况。
从体验上来说是不合理的。</li>
</ol>


<p>针对这种情况可以采用消息推送+拉取的方式来处理。
配对成功后会在被匹配方clientkey下标记匹配对方等到用户下次重连之后在新
的心跳里可以直接获取到被匹配的对象。</p>

<p>处理方式：
因此后台增加了一个新逻辑。A和B配对上了，就从队列中剔除。同时将A，B的配对
信息存起来。扩充存到
clientkey：key：clientkey，value：bool
这个数据结构里即可，上述数据结构变为
clientkey：key：clientkey，value：bool，配对上的uid
每配对上一次，都要记两个clientkey的数据，clientkeyA和clientkeyB
每次用户的心跳信息上来后，都会预先查看自己的clientkey下是否已经有配对信息了。
如果有，则返回给用户配对信息。如果没有，则走正常逻辑。依靠push和拉取
两种方式保障push下的体验问题。</p>

<p>如下所示</p>

<p>被匹配上的B push虽然接收失败了，但是重新连上来之后带的还是断线之前用的clientkey。
发送的心跳到达后台查下clientkey下面已经有匹配信息了，本次匹配过程就正式结束了。</p>

<p>解决完上述的问题，原本简单的匹配过程变成了下述的流程</p>

<p>心跳主体流程图如下：</p>

<h4>正常心跳处理流程：</h4>

<p>用户首次进入话题和心跳都走同一个协议
1. 拉取用户个人资料
2. 拉取话题等待队列
3. 查看队列中是否有人在等待。根据用户条件做匹配。如果配对上则将匹配对象出队列，
通过push  svr 给双方发一个配对成功的push  请求。如果匹配不上则查看是否在队列中，
不在则将用户存入队列，在则更新心跳时间戳。匹配过程校验用户的心跳时间戳，
如果过时则将用户清出排队队列。</p>

<h4>退出话题流程：</h4>

<ol>
<li>拉取话题的等待队列</li>
<li>将用户从队列中剔除。</li>
<li>设置clienkey 下的bool值。</li>
<li>话题的等待人数减一</li>
</ol>


<h4>快速推荐流程：</h4>

<p>向用户推荐话题是在用户等待多次之后没有匹配上，在心跳返回包里给用户返回
可以快速匹配到人的话题。用户达到某个心跳阀值后后台会快速推荐流程。心跳计数放
在attachinfo里，根据终端透传回来的用户计数来触发快速推荐逻辑，快速推荐整体流程如下：
1. 拉取用户个人资料
2. 拉取所有的话题列表
3. 根据用户属性和话题属性筛选话题。
4. 后台会维持每个话题前一个小时和当前小时的配对成功计数。作为话题配对速率。
根据配对速率和当前队列的等待人数做加权排序，取前两个。如果匹配不上则返回空包。
给用户推荐的话题会缓存到终端上，同时会在心跳协议的attchinfo里带上标志数据，
下次心跳来了就不再走推荐逻辑了。心跳协议里有attach_info 字段，通过客户端透传，
加入心跳计数，推荐的话题标志位。下次后台拿到后根据心跳计数和是否已经有推荐话题
标志来决定是否走快速推荐的话题流程</p>

<h5>从这个独立系统里获得的收获是：</h5>

<ol>
<li>移动网络下如何结合网络情况最大化的保证用户体验的完整性和逻辑的统一。</li>
<li>需要在请求之间携带关联数据可以使用attch info 透传字段，类似于web的cookie 形式来记录一些关联数据</li>
<li>业务技术选型可以根据访问的数据模型来确定。不要一开始就把思维给定死了。</li>
</ol>


<hr />
]]></content:encoded>
    </item>
    
    <item>
      <title>陌生人群组项目架构浅述</title>
      <link href="http://192.168.73.128:4000/2015/12/15/%E9%99%8C%E7%94%9F%E4%BA%BA%E7%BE%A4%E7%BB%84%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84%E6%B5%85%E8%BF%B0/"/>
      <pubDate>2015-12-15T00:00:00-06:00</pubDate>
      <author>chriswei</author>
      <guid>http://192.168.73.128:4000/2015/12/15/陌生人群组项目架构浅述</guid>
      <content:encoded><![CDATA[<hr />

<p>   去年还在做社交产品后台。过了不到半年原部门就被解散了，如今入职游戏已经半年有余了。
昨天跟一个技术总监闲聊，他问了我下过去都做过什么。摸摸脑袋竟然有好些细节都遗忘了。
趁着入职新项目组还有一周的交接时间，把过去的做过的服务挑出来。做下陈述总结。</p>

<hr />

<hr />

<p>   去年做过的服务是一个陌生人群聊的服务。主打兴趣社交，有群组聊天，有lbs 活动，
有话题速配聊天等服务。而我恰好都接触一些。 今天就先简单陈述下后台的整体架构和涉及到的一些细节.
服务器的设计和部署是冲着海量用户去的。在北京，上海，深圳三地都设置有接入点。处理后台集中部署在上海。
陈述前先细述下产品的策略架构。如下所示：
<img src="http://7xp7tl.com1.z0.glb.clouddn.com/%E5%BE%AE%E7%BE%A4%E7%BB%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A4%A7%E7%95%A5%E6%9E%B6%E6%9E%84.png" alt="群聊粗略架构" /></p>

<hr />

<h3>首次登陆处理流程是这样的：</h3>

<hr />

<ol>
<li>客户端会写死一个域名。登陆的时候会访问域名所在的Mapping Server 映射服务器。
映射服务器会根据用户传上来的ip，网络运营商信息来给用户推荐最优接入的服务器ip列表
（统一接入网关的服务器ip）。这些最优接入的ip是由玩家跑马分析出来的，这里不做展开。</li>
<li>客户端在拿到最优接入ip之后，发起tcp连接操作。连接上统一网关之后。统一网关会将
用户请求转发给长连接服务器。</li>
<li>长连接服务器本地会做一些简单处理（黑名单用户过滤，鉴权）。鉴权通过之后会将请求转发给轻逻辑服务器。
轻逻辑服务这一层有多个服务器。转发给哪台服务器在长连接服务器这里会有一个决策逻辑，
当然这个决策逻辑最好是配置化的，具体算法由独立模块负责。（长连接服务器需要稳定少升级）
长连接服务器采用的libevent来管理网络连接。</li>
<li>轻逻辑服务器主要是做数据装配。各种版本兼容的擦屁股活。如果接口逻辑不重，
那么接口的实现也会做在这一层。时间久了代码看起来像一坨屎。这一层蛮要求编码规范约束和框架约束的，
否则到后面只想重构根本不想在上面继续迭代。轻逻辑层是面向用户最近的业务层，
通信上采用protocol  buffer协议，会负责请求各个api然后将客户端要求的数据组装起来返还给用户。</li>
<li>如果接口逻辑比较复杂，涉及到多模块的数据处理，这时候就会按照功能来划分模块。
每个模块负责实现独立特定的业务逻辑。这些业务逻辑属于重型svr这一层。为了避免不同业务之间相互干扰，
这一层不同功能模块的服务都是分开部署的。如果混合部署会出现各种相互干扰的问题。
（某些功能模块做了运营活动或是具有节日特征，会导致请求量激增。如果混合部署可能会导致其他模块拥有的计算资源被干扰，
一个稳健系统的一个重要条件就是柔性可用，某个模块有问题不会波及到其他模块）</li>
<li>数据层采用的是高速key/value 存储。如果部署在同一个园区内。从逻辑层到数据层的访问时延 一次往返在10ms内，基本可以满足业务的需要。</li>
</ol>


<hr />

<hr />

<pre><code>请求流向有两个：
</code></pre>

<ol>
<li>从长连接服务器->轻逻辑层服务->重逻辑服务<->db.常规请求类似于拉取个人资料，群资料会按照这个路径来走并原路返回</li>
<li>聊天消息会触发一个在线的推送处理。假设A用户在群里说了句话，由此引发的请求流向路径如下:
A用户接入机->轻逻辑层服务－>重逻辑服务层（聊天svr，做群聊计数修改，权限校验，敏感词检测，数据落地）
->push  svr(查找在线列表，获取用户接入的长连接服务器地址，组装推送包)->各个用户的接入机->到达终端</li>
</ol>


<hr />

<h3>以上架构中有几个细节不得不提:</h3>

<hr />

<ol>
<li>轻逻辑层和重逻辑层不缓存任何用户数据，每个请求到来之后都是立马从数据库拉取最新数据进行处理。状态只保存在数据层。这种处理方式的好处是可以无障碍平行扩容。全程不需要开发介入，运维改改配置就能迅速增大系统的处理能力</li>
<li>第一点提到的逻辑处理层无状态会带来数据一致性的问题。对于一些共享数据访问可能会出现冲突的问题。但是社交应用不像银行系统需要高度的一致性。因此综合可维护性，性能采用了最终一致性的数据处理方式。处理数据的时候采用cas版本号校验。一旦出现冲突，让请求失败即可。这种失败的概率很小，对玩家体验造成的损失几乎可以忽略</li>
<li>为了提高系统的鲁棒性。层与层之间进行请求转发的时候必须摆脱对于物理地址的依赖。举个例子，轻逻辑层有多台服务器，重逻辑层也有多台服务器。但是轻逻辑层的服务器对后端的重逻辑层服务器发起一个网络请求的时候，只会选择一台。最简单的处理方式是维护一堆物理ip，每次负载均衡出一个ip。然后连接。这种方式最大的弊端就是后端服务器挂了会影响线上的请求。等到告警短信来了再手动刷配置下掉服务效率慢不说，也影响大家的日常作息。</li>
</ol>


<hr />

<h3>业界的经典处理方法如下:</h3>

<hr />

<pre><code>在主调和被调之间加一层proxy代理层。主调方只要无脑转发给proxy层就行。
</code></pre>

<p>Proxy这里会进行负载均衡 并会检测每个请求的错误情况。如果是非逻辑错误超过
一定阀值，或者定期发送给被调方的心跳没有及时 收到回包，就会告警并且自动将
被调方的故障服务器自动屏蔽一段时间，过段时间再重试下决策是否重新加入服务列表里。
<img src="http://7xp7tl.com1.z0.glb.clouddn.com/proxy.PNG" alt="代理转发" /></p>

<pre><code>不过项目采用的是我见过的最优雅的处理方式。也就是上图架构中 的虚拟地址到物理地址的映射服务。
</code></pre>

<p><img src="http://7xp7tl.com1.z0.glb.clouddn.com/L.PNG" alt="虚拟地址到物理地址转换服务" /></p>

<hr />

<h3>工作机理是：</h3>

<hr />

<pre><code>在每个服务器上维护一个agent。这个agengt负责接受映射中心推送过来的映射结果。
</code></pre>

<p>映射结果 是key=》ip，port。 主调方只要采用调用映射服务的api，传入key，
可以自动获取到ip，port。这个ip和port 是agent 计算出来的结果， 兼顾了负载均衡的逻辑。
每个请求调用结束之后都会上报业务错误码给agent，由agent 进行决策看是否是后端某个服务出故障了。
当然agent也会定期发送心跳给后端服务器。Agent得到的服务器检测结果会上报给映射中心。
映射中心再统一决策修改映射规则并下发给各个agent。以上处理方式把传统的proxy从通信链路里剥离出来。</p>

<hr />

<hr />

<pre><code>主调方采用滞后反馈的结果才连接后端，这种处理方式将负载均衡的功能处理成旁路逻辑。
</code></pre>

<p>提升了性能，而且后端的运维升级完全对前端来说事透明的。 当然有人会说以上的架构不就是接入层，
逻辑层，数据层这套东西吗。凭什么你这套架构就能够处理海量服务。但我想纠正一点
海量的服务体现在系统的各个细节处理上。而不是这些大概的层级划分。</p>

<hr />

<hr />

<pre><code>从系统处理上，每一层都有精细化的处理。举几个简单的例子：
</code></pre>

<ol>
<li>在接入层这里会做最大连接数限制.</li>
<li>轻逻辑层这里的网络连接采用的是proxy，worker的处理模型。请求会以队列的形式进行排队处理。进队列的时候都会打上一个时间戳。如果超过队列缓存的最大个数，新来的请求会直接丢弃。如果从队列中获取到的请求等待时间过长也直接丢弃。</li>
<li>客户端这里也会谨慎重试。如果发现后端返回的错误码显示过载，会采用N步回退的重试策略，或者是出动画或者提示降低用户的焦躁感避免疯狂重试。</li>
<li>对于每个请求的成功，失败，返回码。时延，错误码分布，时间，qua这些维度的信息，都会在框架api里进行统一的封装上报。可以在监控平台里快速查询到接口在某个具体时间段的处理情况。如果超过一定阀值会进行短信，微信轰炸，第一时间通知到责任人</li>
<li>对于每台服务器都有流量，CPU，磁盘IO监控。对于每个涉及到用户关键数据的修改请求都有流水记录。方便出现问题的时候快速获取上下文处理。

<ol>
<li>对于功能相近的请求做抽象，进行并包处理。减少网络请求的耗时（国内跨地区的时延往返在100ms左右，不算上网络延迟堵塞这种情况）</li>
</ol>
</li>
</ol>


<hr />

<hr />

<h3>系统设计关键点：</h3>

<h4>1.用户在线列表是如何维护的</h4>

<p>上海深圳北京三地用户接入，都会设置用户的在线状态值。这个标记值存储在上海的db中，
同时存储接入机的ip和端口地址。此后，客户端会定期的发送心跳给接入机，接入机透传
给推送svr。这个心跳的间隔大概在5分钟到半小时不等。一旦推送svr在超过两个
心跳以上的时间间隔里没有收到客户端的心跳数据，就会认为是客户端掉线了。
这种情况下推送svr会设置db中的用户在线状态为 下线，同时清理纪录的接入机的
ip端口数据。用户群聊的时候会批量拉取群内用户的在线状态, 同时获取用户的接入机ip和端口，
除此之外还会读取最后一次刷新的心跳时间。 只有在线状态被标记
并且心跳时间的才会执行推送。以上对于用户的在线状态数据是由接入机进行通知维护的，一旦接入机挂机。就只能等
用户重连到新接入机再进行更新。所以，在线状态数据中还有一个辅助手段是用户的心跳时间，推送的时候一旦
判断到用户的最后一次心跳时间超时，也会设置用户下线并且不执行推送
设置这么长的一个心跳间隔也是有原因的。移动网络下用户可能会经历频繁的网络切换，
信号丢失等情况。预留足够的经验值时间来等待用户重连。目前这种机制推送的成功率在95%左右。</p>

<h4>2.用户在线列表是如何存储的</h4>

<p>用户在线列表存储在一个内存级的key，value 数据库中。用户可以选择用memcache，也可以用redis。做好备份，避免单点故障。</p>

<h4>3.前端接入机出故障会对用户有什么影响，如何处理可能出现的风险</h4>

<p>前端接入机是有状态的，一旦挂机，所有通过此接入机的用户都会掉线。这种情况下统一接入网关是有探测的。
会第一时间把故障机子下掉。用户重新登录接入该网关的时候，会自动将请求发送给其他接入机。从用户体验
上来说这种影响十分有限. 没有执行重新登录的用户，此时实质是在线不可达的。如果刚好有推送请求恰好读取了
用户过期的在线状态，并且还处于心跳时间间隔内。这种情况推送是会失败的。用户侧最直观的感受就是没有收到
推送的消息。作为辅助手段，在后台会存储用户的未读消息计数。等到下次玩家登录重连的时候会上传本地的已读消息
计数。通过比对之后再把没有拉取到的数据用主动拉取的形式进行同步</p>

<h4>4. 轻逻辑层，重逻辑层服务器挂机</h4>

<p>在这一层有虚拟地址映射的监测服务，如果请求失败超过一定阀值，会自动将故障服务器从可服务列表里除名。
会影响一些用户的部分请求。由于这些层的服务器无状态，下次请求的时候会使用正常的服务器来处理.对用户来
说几乎是无感知的。</p>

<h4>5. 数据层挂机</h4>

<p>数据层采用的是分布式的架构。实际服务的机器不止一台。分布式本身设计的核心理念就是容错处理</p>

<hr />

<hr />

<pre><code>以上处理方式适用于社交相关的应用。总体思想是先扛住再优化,架构上支持快速扩容,
</code></pre>

<p>柔性可用。在有限的资源支撑下快速应对随时会爆发的互联网海量请求服务。</p>

<pre><code>写得太书面了，最近文笔退却好多争取以后写得幽默风趣点啊！
</code></pre>

<hr />
]]></content:encoded>
    </item>
    
    <item>
      <title>vim读书笔记</title>
      <link href="http://192.168.73.128:4000/2014/10/21/vim-read-book-note/"/>
      <pubDate>2014-10-21T00:00:00-05:00</pubDate>
      <author>chriswei</author>
      <guid>http://192.168.73.128:4000/2014/10/21/vim-read-book-note</guid>
      <content:encoded><![CDATA[<hr />

<p>以前整理过vim的读书笔记，有本书是一位在读的印度学生写的， 写了些常用
的使用技巧，不像vim  用户手册那么大而全，非常适合用户自己
日常工作的使用，以下指令就可以高效完成90%的文本编辑工作了。
当然只是一些速记笔记，掌握vim的人可以时不时翻一下，以便回忆起一些
很久没用到有点生疏的指令</p>

<h3>跳转</h3>

<hr />

<pre>
<code>
Normal mode

:x   save working files and quit
:wq 
 ZZ 
 :qa   Exit all open files
 :version
 vimtutor
 [Ctrl] +  F
 [Ctrl]  + B
 [Ctrl]  + E
 [Ctrl]   +  Y

 jump :w  W  e  E  b B 
 {
 } 
 ( 
 )
 [[ 
 ]]

 H  M   L  (screen  jump)


 50% (Go to the 50th percentage of file)
 :50   (Go to the 50 th  line)
 50gg   (Another way to jump to 50 th line)


 [(  (Go to the previous unmatched (    )
 [])   (Go to the previous unmatched  )    )
 [{    (Go to the previous unmatched  {    })
 []}     (Go to the previous unmatched  )]]   )


 [Ctrl]  + O (jump back to previous spot)
 [Ctrl]   + I   (jump forward to next spot)


</pre>


<p></code></p>

<h3>搜索</h3>

<pre>
<code>
 vim +?search-term  filename   (Go to the first of the specified search term from bottom)
 vim +/search-term  filename   (Go to the frist of the specified search term from top)
 vim -t TAG  (Go to specific tag)


</pre>


<p></code></p>

<h3>打标记</h3>

<hr />

<pre>
<code>
 marks:
 ma
 `a
 'a
 :marks
 
 ma,mb,then  :'a,'bs/old/new/gc
 will display specific area  word
 
 
 
 `"  (To the position where you did last edit before exit)
 '.  (To the position of where the last change was made)
 
 
 
 ddp  dwp  
 

 :r!   COMMAND (Insert output of a command into current file after the current line)

 Insert Mode
 SHIFT + <Right  Arrow >  Go to right word -by-word  in insert mode
 SHIFT + <Right  Arrow >  Go to left word-by-word  in insert mode
 
 </code>
 </pre>


<h3>复制</h3>

<p> <code>
 <pre>
 copy line to the clipboard</p>

<p> :%y+
 :y+
 :N,My+</p>

<p> To copy the visual selected line to the clipboard,first visually select the
 lines, and :y+ which will apppear as :'&lt;,'>y+</p>

<p> here please check if your vim version support y+　register</p>

<p> write part of File to another File</p>

<p> visual mode selected save area,  then :
 :w newfilename</p>

<p> also you can do like this:
 :5,10w  newfilename</p>

<p> swap line or character</p>

<p> :xp   (swap character)
 :ddp  (swap line)</p>

<p> (dot)  usage:
 1. Search for a string in a file using: /zhihao
 2. Replace zhihao with wzhihao  using: cwwzhihao<Esc>
 3.Search for the next occurrence of zhihao:n
 4.Replace zhihao with wzhihao using:.(dot)</p>

<p> </code>
 </pre>
 ###正则处理
 <pre>
 <code>
 :g usage
 :g/^$/d  (delete all empty lines in the file)
 :g/^\s*$/d  (delete all  empty  and blank lines in the file)
 :g/pattern/d  (delete )
 :g/pattern/. w >> filename  (Extract line with specific pattern and
 write it into another file)(very userful!!!)
 :g/^/m0  (reverse a file)</p>

<p> :g/^\s<em>PATTERN\exe "norm! |/</em>\<ESC>A<em>/\<ESC>"  (Add a C style comment {/</em>text */} to all lines matching the pattern)
 (it can not work in gvim  yet,though it seems to be so useful  ~~~~)</p>

<p> Copy Lines to Named Buffer for Later Use
 valid named buffer: a to z (26 total valid named buffers)</p>

<p> "ayy
 "a5yy  (copied 5 lines to buffer "a")
 "ap  (Paste copied lines from buffer "a"  after the cursor)
 "aP  (Paste copied lines from buffer "a"   before the cursor)</p>

<p> </code>
 </pre></p>

<h3>头文件预定义</h3>

<p> <code>
 <pre>
 lowcase to uppercase,normally used in header file define
 visual mode
 select replace area
 U  (to upper case)
 u   (to lower case)</p>

<p> sort file content from vim as below
 :sort</p>

<p> sort selected content
 visual mode select specific area and add !sort at the end
 :'&lt;,'>!sort</p>

<p> :sort !  (descending order)
 :sort i (sort ignore case)
 :sort u (remove duplicate lines)</p>

<p> </code>
 </pre></p>

<h3>创建酷炫的文件注释</h3>

<p> <code>
 <pre>
 Extremely useful!  (78)</p>

<p> create a new *.c  file with automatic header</p>

<p> cat  c_header.txt
 :insert</p>

<p> /*-.-.-.-.-.-.--.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.</p>

<pre><code>     File  Name:

     Purpose:

     Creation Date:

     Last Modified:

     Created By :
</code></pre>

<p> -.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.*/</p>

<p> add following lines in ~/.vimrc  file</p>

<p> autocmd bufnewfile <em>.c  so  /chriswei/c_header.txt
 autocmd bufnewfile </em>.c  exe "1," . 10 . "g/File  Name : .<em>/s//FileName : " .expand(%s)
 autocmd  bufnewfile </em>.c  exe "1,"  .10 . "g/Creation Date :.*/s//Cteation Date: " .strftime("%d-%m-%Y")</p>

<p>  autocmd Bufwritepre,filewritepre  <em>.c  execute "normal ma"
  autocmd  Bufwritepre ,filewritepre </em>.c exe "1," . 10.
  "g/Last Modified:.<em>/s/Last Modified:.</em>/Last Modified:" .strftime(%c)
  autocmd bufwritepost,filewritepost *.c execute "normal `a"</p>

<p> </code></p>

<h2> </pre></h2>

<p> <pre>
 <code>
  editor  color
  :syn on
  :syn off</p>

<p>  (extremely useful!)
  in vim Press K  on the word for which you want to read the man
  page
  K</p>

<p>  (extremely useful!)
  gd  (Go to the local declaration of a variable)</p>

<p>  gD (Go to the global declaration of a variable)</p>

<p>  (extremely useful)
  [Ctrl]  -  A  (incre Number)
  [Ctrl]  -  X   (Desc Number)</p>

<p>  (extremely useful!)
  to execute single Vim command  in insert mode</p>

<p>  [Ctrl]  +　Ｏ</p>

<p>  you are in insert mode typing characters
  Press  Ctrl - O ,which will terporarily take you to command mode
  do some vim command
  then you will automatically back in insert mode after the single vim commands is executed</p>

<p>  [Ctrl]  +  G  view file details</p>

<p>  </code>
  </pre></p>

<hr />

<p>  <code>
  <pre>
  gUU  change all the visual selected area to uppercase
  guu change all the visual selected area to lowercase</p>

<p>  execute any vim command when opening a file</p>

<p>  vim -c '<command   1>'  -c '<command 2>'  <filename></p>

<p>  skip loading plugins temporarily
  vim --noplugin filename.txt</p>

<p>  change color scheme</p>

<p>  :colorsheme  [color scheme]</p>

<p>  (extremely useful!)
  gf  (open a file whose names is currently under the cursor)</p>

<p>  </code>
  </pre></p>

<p>  ###vim加密文件</p>

<p>  <code>
  <pre>
  (safe   encrypt  extremely useful)
  :X</p>

<p>  unencrypt
  :set key =
  (to remove encrypt key)</p>

<p>  (extremely useful)
  save and resume vim sessions
  :mksession  filename</p>

<p>  vim -S filename</p>

<p>  </code></p>

<h2>  </pre></h2>

<p>  <code>
  <pre>
  (extremely useful)
  vimdiff  filename.txt  filename.txt.backup
  when diff window open
  [c  (Go to the next change inside vimdiff)
  ]c  (Go to the previous change inside vimdiff)</p>

<p>  Page 124 is Useful</p>

<p>  (extremely useful)
  :vimgrep search-pattern   filename.*
  (search for the search pattern inside all files ending  in .txt in the current directory)
  vimgrep by default will jump to the first file that contains a match,
  :cn (will jump to the next file)
  :clist    (view all files match the pattern)</p>

<p>  set vim as default editor as follow
  export EDITOR=vi</p>

<p>  view all changes done to a file after opening it
  :changes</p>

<p>  view ascii code of a charactor
  ga</p>

<p>  (extremely useful)
  vim -b  binaryfile (edit binary files in vim editor)</p>

<p>  </code>
  </pre></p>

<hr />

<pre>
<code>
  terrible  useful，release  your  mouse：
  paste  content  copy  by  yw  in  command line
  1.  “ayw   copy  the  search  content
  2. Ctrl-R  and  input  a    to paste the content
  detailed introduction  below:
  There are times when you're tempted to lift your hand from the keyboard 
  to the mouse, idly wondering if there's a better way. One such case is
  taking text from a buffer and placing it into Command-line mode. For example,
  performing text substitution with %s, or invoking a shell command with :!.
  Many Vim users will reach for the mouse and use the operating system's copy
  and paste feature to do this, but there's a quicker way provided by Vim's registers.

  The CTRL-R (:help i_CTRL-R) command can insert the contents of a register in Insert 
  or Replace mode. This is known as a "special key" (:help ins-special-keys). 
  The great thing about this shortcut is you can reuse it to put registers into
  Command-line mode. For example, let's say you've got some text you want to search 
  for in a buffer. First yank the text into a register, and then paste it with CTRL-R:
  In Normal mode, type "ayw to yank a wordPress escape, and then / to searchThen press
  CTRL-R and a to put register a
  A shorter way to do this is to use the default register. Typing yw will yank up
  to the word boundary into the default register, and then typing CTRL-R_" will put 
  it into the command-line. It's worth practicing using this, particularly if you 
  haven't got used to working with registers yet.
  
  (extremely useful!!!!!)
  folder code
  za (Toggle the fold under the cursor)
  zR (unfold all folds)
  zm (Fold everything back again)
  :set foldmethod = manual
  
  zf/pattern  (to fold lines selected by the search pattern)
  
  :range fold  (To fold lines specified by a range)
  :mkview (save all your folds as show above)
  
  :loadview
   

</code>
</pre>


<h3>推荐插件</h3>

<hr />

<pre>
<code>
   plugin

    ctags
    :ta main  (jump to the definition)
    :ta /^get   (jump to the match pattern)

    NERD  Tree  (specially useful)

    autocorrect.vim  (nice)

    (extremely userful!!!)
    Align.vim

    before align 
    a = 1
    hello world = 2
    sh = 3

    after align 
    a           =  1
    hello word  =  2
    sh          = 3 

    perform 
    visual  mode select align area and input
    :'<,'>Align = 

</code>
</pre>

]]></content:encoded>
    </item>
    
    <item>
      <title>类微博类产品mysql存储方案设计.</title>
      <link href="http://192.168.73.128:4000/2014/10/18/like-microblog-mysql-table-design/"/>
      <pubDate>2014-10-18T00:00:00-05:00</pubDate>
      <author>chriswei</author>
      <guid>http://192.168.73.128:4000/2014/10/18/like-microblog-mysql-table-design</guid>
      <content:encoded><![CDATA[<hr />

<p>下午需求做完了，刚好其他同事在搞方案评审。做的是一个新的sns产品。所以去旁听了会。sns中涉及到好友动态查看，以及互粉关系链存储，评论这些功能。由于是节目订阅类， 产品的功能点有很多跟微博类似。之前使用公司的架构同样可以扛住海量用户的请求冲击。但是leader的想法是追求创业团队的小而美，快速迭代上线。用公司的框架太重了， 调用链路太长，开发和维护的成本都比较高。所以架构上采用了通用的lamp组件进行开发。问题来了，挖掘机哪家强.......哦，说错了，是mysql中的数据结构如何组织才能够支持 快速的查询和数据层的无障碍扩容。</p>

<hr />

<p>关系链中因为有关注，有粉丝，最简单粗暴的方式是把这些关系都存成一张表。表属性设置为fromid，toid，timestamp。fromid代表发起关注者的id，toid表示被关注对象的id。 我们暂时称这张表为A吧。如果要查一个人关注了谁，<pre><code>select toid from tablename where fromid=XXXX</code></pre>就行。如果要查一个人的粉丝是谁<pre><code>select fromid from tablename where toid=XXXX</code></pre>就可以检索出来</p>

<hr />

<p>这么搞真的行么？用户获取好友动态的时候，不管是采用推送模式还是拉取模式，都需要去查一把关系链。推送模式得查粉丝关系链，拉取模式得查关注关系链。 进一个人的主页查看对方关注列表或者粉丝列表需要知道这些列表中的账号与自己的互粉关系同样需要查询这张表。可以说关系链是访问最频繁的数据之一。 对于百万级注册用户，假设平均每个人关注了100个人，那这里的关系记录有100个百万也就是1亿条记录。对于检索频繁而且数据量这样大的数据，存到一个表里对于mysql的性能 是个问题。业界常用的做法是分库分表。分表也有垂直分表和水平分表这些方式。对于关系链存储，最适合的是水平分表。将数据打散到多个表中来减低访问的压力。 最简单的分表方式可以按照fromid来hash。业务侧或者中间层维护好映射关系。</p>

<h3>这里又带来了另一个问题：</h3>

<hr />

<p>数据分表之后。如果要查找某个人的粉丝列表<pre><code>select  fromid  from tablename  where toid=XXX（XXX是用户id）</code></pre>，需要对所有表都发起sql查询。假设分了N张表。 有M个查询粉丝列表的请求那这样产生的请求量就有M*N个sql请求。拿这样的系统出去都没法见人啊。说白了就是查反向关系链成本比较高。解决方法是再开一个冗余的关系数据表。 表结构跟A表一样，我们暂时称作B吧。但是分表的时候是根据toid来进行水平分区的。查反向关系链的时候可以根据toid映射到对应的表里快速查找到用户的粉丝列表。</p>

<h3>采用这种方式又带来新的问题：</h3>

<hr />

<p>用户的一次关注操作需要在表A和表B中塞一条记录。这里本身是一个事务操作。当然我们可以以A表的数据为主。B表数据只要保证最终一致，定时同步就行。</p>

<h3>这样的关系链存储是否就是最优的？</h3>

<hr />

<p>想得美啊骚年。遥想当年使用微博，知乎的时候，刚注册就给你推荐一坨人，运营和产品要搞个一键关注，一关注就是几十上百个人啊。 采用这种存储方式带来的问题就是关注多少人就产生多少sql语句啊。而且因为搞了A，B两张表，就是2倍的sql语句啊。虽然关注的写量不比发SNS里发动态的写量 ，但是这里的数据表膨胀速度还是蛮快的。尤其是有高质量的用户活跃的时候，关注写操作比较多，半夜都要起来分表啊，做数据迁移。写操作的成本和维护的成本都比较高。 是否有一种解决方案可以减少sql的执行次数，降低数据表膨胀的速度？毛主席教导我们，把别人的经验变成自己的，他的本事就大了。 qq空间老版本用的也是mysql。关系链存储存成两张表，一张是follow关注表，一张是粉丝表（这里简称为方案二）。表结构属性如下：uid，uidlist。 uid是用户的id，uidlist是关注对象或者粉丝列表，打包成二进制数据存成BLOB。</p>

<h3>采用这种方式，好处有以下几点：</h3>

<ol>
<li>原来一键关注多人的写操作可以整成一条语句，只要修改uidlist就行了。另外数据会更加紧凑。假设mysql存储采用B+树InnoDB 存储引擎，平均每人的关注和粉丝列表都是100人，采用Blob的方式B+树的宽度要比原来存到A，B表（这里简称为方案一）的宽度理论上小100倍。B+树的结点分裂也比方案一频率要小。在uidlist比较短的时候写入效率高。</li>
<li>检索效率高。只要检索到用户的id就可以把所有关注或者粉丝列表拉出来，而方案一由于是一条条记录，即使在fromid或者toid上建立索引。查找的时候需要拉出
一个或者多个区块的数据出来。</li>
</ol>


<hr />

<p>缺点：采用这种方式，当用户关注对象或者粉丝量大的时候，每次都要操作一坨数据。如果某个用户出现粉丝集聚的情况，每次更新该用户的数据的时候都要把整个粉丝list都拉出来， 再塞回去。如果uidlist里需要塞每个用户的操作时间戳，uid，就有8个字节。出现百万乃至千万个uid的时候每次拉取的数据会达到8M-80M。在删除的时候还要遍历这些uidlist再行删除。 判断用户是否在uidlist中也要做次百万千万级别的查找。为何空间适用的关系链存储就没问题。因为空间是熟人社交，没有粉丝名人的集聚效应。当然你会说不对啊， 小米的认证空间不就有上千万粉丝么。这里是因为认证空间采用的是B+树单纯存储这些关系链的，相当于又采用方案一的表结构来存储。没有哪种数据组织形式可以适用于一切情况。 业务侧个人的关注量很有限，但是粉丝数却有可能突破上百万上千万。我们可以采用混合方案的方式。对于粉丝数目超过百万的大户，我们可以采用方案一的结构来单独存储这些用户的粉丝关系链。
粉丝数目小于百万的，可以采用方案二来存储。方便数据的快速写入和读取。这种处理方式可以应对海量的关系链数据处理，但是无疑也相应的增加了业务逻辑的复杂性。但是相对于采用单纯的方案要好</p>

<hr />

<p>SNS中涉及到另外一个问题，好友动态如何快速的拉取。这里有pull和push两种方式。pull主要是采用拉取的方式。用户每发表一个好友动态，都会插入到一个动态表中，我们叫做feeds吧。feeds的表结构设置为 <pre><code>author，content，timestamp,extra msg</code></pre>每次拉取的时候需要</p>

<pre><code>select * from feeds where feeds.author in(关注列表)</code></pre>


<p>pull方案既要查询关系链，又要根据关系链去动态列表中过滤出关注对象的动态。做排序。检索的时候可以根据在增量feeds表中做数据检索。 这种方式实现简单，可以实现业务需求，但是操作时间复杂度达到O(M)*O(N),M是动态表中的记录长度，N是关注列表的长度。</p>

<hr />

<p>push方式是再设置一张表，我们叫做friendfeeds吧，表结构设置为uid，feedslist[{feedsid1 : timestamp1},{feedsid2:timestamp2}] ，uid是用户id，feedslist是关注对象发的feedslist列表。 列表按照发布时间进行排序。feedslist可以只存好友动态的id。如果用户取消了对关注对象的关注，顺带删除feedslist中关注取消对象的动态信息就行了。用户刷新动态的时候只要查friendfeeds， 拉出feedslist。如果feedslist中存储的是动态的全量数据，那么拉取好友动态的时间复杂度是O(1).但是如果出现粉丝集聚，热点账号发布一条动态会带来恐怖的写扩散。上千万用户会带来上千万的写操作。 而且带来大量的数据冗余。用户修改动态数据或者app需要修改展现格式的时候修改成本也很大。退一步feedslist只存feeds的索引，拉取到feedslist之后再根据索引去拉动态数据。所有动态数据只存一份。 这种方式带来的拉取时间复杂度是O（M）（M是feedslist的长度）要修改动态的展现形式或者用户需要修改自己发布的动态的时候都只要动一份数据就行了。 只是只存索引的方式同样还是没法应对粉丝集聚带来的恐怖写扩散问题，之前处理过的相关的业务，单单针对热点账号存的写索引数据成本很惊人。</p>

<hr />

<p>这里进一步做混合方案。push只针对热点用户，用户每次登陆的时候，都会去更新热点状态位，对于长时间没有登录的用户，热点状态位被重置。用户很长时间登录之后如果热点状态位没有被激活， 就用拉的方式去获取动态数据。另外如果查询到的热点状态位处于有效状态，则去friendfeeds表中拉取自己的好友动态。对于热点账号， 为了防止恐怖的写扩散,我们可以在friendfeeds数据表中加入新的属性如下：<pre><code>uid，content，feedslist[feedsid1,feedsid2....]，hotuidlist[uid1,uid2....]，timestamp</code></pre> feedslist存储的是关注人最近发布的动态id，按照发布时间排序。hotuidlist是自己关注的热门账户uid 列表。timestamp是用户上次拉取动作的时间戳。 另外有个hash表（简称动态时间hash）存储热点用户的最新的动态发表时间戳，最近一周内发布的动态的id和对应的时间戳，hash表结构表示为：</p>

<pre><code>key：uid
value：最新动态时间戳，[{feedsid1:timestamp1},{feedsid2:timestamp2}.........]</code></pre>


<p>每次热点用户发布新动态的时候都会去更新这张hash表的发表时间戳和id。用户在拉完friendfeeds   里普通关注人的feedslist之后，也会拉到关注的热点账号的uidlist和上次拉取的时间戳。 拿这个时间戳去动态时间hash表中比对下时间戳，如果不一致就根据时间戳过滤出hash表中的这些热门账号的feedsid，再把拿到的这些feedslist索引去feeds表中拉取真正的动态数据。 这样拉取动态的时间复杂度是O（M+N*T/2）M是feedslist长度，N是用户关注的热点账号，T是每个热点账号还未被用户拉取的动态数量。采用这种方案避免了push的写扩散， 相对pull先去查关系链再去遍历feeds表要快些，少了查关系链的sql操作。用户长时间没有登录，后台的定时任务可以重置热点状态，同时删除friendfeeds中关于用户的feedslist数据。 friendfeeds  和hash  这两个结构建议放在cache里，承载的组件可以用redis。</p>

<hr />

<p>当然，对于一个新业务，快速上线才是王道，怎么简单怎么做。当请求量大之后架构上可以支持快速加机器扛住请求。做复合优化方案不需要对存储做大的变动。 个人觉得就是好的方案。第一次发文，有错误的地方欢迎拍砖~</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
